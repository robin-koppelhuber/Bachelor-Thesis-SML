# @package _global_
benchmark:
  name: proof_of_concept
  description: "POC benchmark for roberta-base model merging"

  # Tasks to evaluate (order matches preference vector indices)
  tasks:
    - ag_news
    - imdb
    - mnli
    - mrpc

  # Preference vectors to test (must sum to 1.0 and match task count)
  preference_vectors:
    # Equal preference
    - [0.25, 0.25, 0.25, 0.25]
    # Focus on each task individually
    - [0.7, 0.1, 0.1, 0.1]
    - [0.1, 0.7, 0.1, 0.1]
    - [0.1, 0.1, 0.7, 0.1]
    - [0.1, 0.1, 0.1, 0.7]
    # Balanced pairs
    - [0.4, 0.4, 0.1, 0.1]
    - [0.1, 0.1, 0.4, 0.4]

  # Evaluation settings
  evaluation:
    batch_size: 32
    num_samples: null  # null = use full dataset, integer = limit samples
    metrics:
      - accuracy
      - f1_macro
      - f1_weighted

  # Execution mode (3 options):
  # - train_eval: Train & cache & evaluate & cache; reuse cached models if they exist (DEFAULT)
  # - train_only: Train & cache; skip evaluation, skip if cached model exists
  # - eval_only: Evaluate & cache only; require cached models (fail if missing)
  #
  # Override cache behavior:
  # - benchmark.force_retrain=true: Ignore cached models, always retrain
  # - benchmark.clear_cache=true: Clear all caches before starting
  mode: train_eval

  # Caching (enabled by default, disable with benchmark.cache_enabled=false)
  cache_enabled: true
  force_retrain: false  # Override: ignore cached models, always retrain
  clear_cache: false    # Override: clear all caches before starting

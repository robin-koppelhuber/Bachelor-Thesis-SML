defaults:
  - _self_
  - benchmark: poc
  - model: roberta_base
  - method: ties
  - dataset@datasets.ag_news: ag_news
  - dataset@datasets.imdb: imdb
  - dataset@datasets.mnli: mnli
  - dataset@datasets.mrpc: mrpc
  - optional cluster: null

# Global settings
project_name: bachelor_thesis_sml
seed: 42
device: auto # auto, cpu, cuda, xpu

# Paths
paths:
  # Global data directories (shared across runs)
  data_dir: data
  hf_datasets_cache: ${paths.data_dir}/raw  # HuggingFace datasets cache
  hf_models_cache_base: checkpoints/base  # Base models (e.g., roberta-base)
  hf_models_cache_finetuned: checkpoints/finetuned  # Single-task fine-tuned models

  # Persistent caches (shared across runs for reuse)
  trained_models_cache: checkpoints/trained  # Cached trained models (.safetensors) - SHARED
  evaluation_cache: checkpoints/eval_cache  # Joblib cache for evaluation results - SHARED

  # Per-run output directory (set by Hydra)
  run_dir: ${hydra:runtime.output_dir}  # e.g., outputs/proof_of_concept/2024-12-06_15-30-45/

  # Run-specific subdirectories (temporary outputs for this run only)
  visualizations_dir: ${paths.run_dir}/visualizations  # Generated plots
  checkpoints_dir: ${paths.run_dir}/epoch_checkpoints  # Epoch checkpoints during training (temporary)
  logs_dir: ${paths.run_dir}/logs  # Log files

# Weights & Biases settings
wandb:
  enabled: true
  project: ${project_name}
  entity: null # Set via .env or override
  mode: online # online, offline, disabled
  tags: []
  group: null
  notes: null
  upload_models: false # Upload trained models as artifacts (can be large)
  dir: null # Will be set to output dir in main.py

# Logging
logging:
  level: INFO
  log_to_file: true
  log_to_wandb: true
  console_format: simple # simple, detailed, json

# Hydra settings
hydra:
  run:
    dir: outputs/${benchmark.name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: outputs/${benchmark.name}/sweeps
    subdir: ${hydra.job.num}

# @package _global_
method:
  name: chebyshev
  class_path: src.methods.chebyshev_ft.ChebyshevFineTuning
  description: "Fine-tuning with Chebyshev scalarization"

  # Chebyshev-specific parameters
  params:
    # Training settings
    learning_rate: 2e-5
    num_epochs: 3
    batch_size: 8
    warmup_steps: 100
    weight_decay: 0.01

    # Chebyshev scalarization
    use_augmented: true  # Use augmented Chebyshev (recommended)
    epsilon: 0.001  # Augmentation weight for smoothness

    # Preference handling
    normalize_preferences: true
    # Compute with 200 samples on fine tuned models
    utopia_point: [-0.13253337144851685, -0.12508374452590942, -4.7180962562561035, -0.24992592632770538]  # null = compute from single-task optima
    nadir_point: null   # null = compute from worst performance

    # Optimization
    gradient_accumulation_steps: 4  # Effective batch size = 4*4 = 16
    max_grad_norm: 1.0

    # Memory optimization
    use_fp16: false  # Mixed precision training (disable for max accuracy)
    dataloader_num_workers: 0  # Avoid multiprocessing overhead
    max_samples_per_task: null  # Limit samples per task (null = use all)
    use_streaming: true  # Streaming datasets (minimal RAM, slower but accurate)

    # Checkpoint management
    save_epoch_checkpoints: true  # Save checkpoint after each epoch
    auto_resume: true  # Automatically resume from latest checkpoint if available
    keep_all_epoch_checkpoints: false  # Keep all epoch checkpoints (true) or only latest (false)

# @package _global_
method:
  name: chebyshev
  class_path: src.methods.chebyshev_ft.ChebyshevFineTuning
  description: "Fine-tuning with Chebyshev scalarization"

  # Chebyshev-specific parameters
  params:
    # Training settings
    learning_rate: 2e-5
    num_epochs: 3
    batch_size: 8
    warmup_steps: 100
    weight_decay: 0.01

    # Chebyshev scalarization
    use_augmented: true  # Use augmented Chebyshev (recommended)
    epsilon: 0.001  # Augmentation weight for smoothness

    # Preference handling
    normalize_preferences: true
    # Set to null to auto-compute from fine-tuned models (recommended)
    utopia_point: null  # Best achievable loss per task (computed from single-task fine-tuned models)
    nadir_point: null   # Worst performance (not currently used)

    # Optimization
    gradient_accumulation_steps: 4  # Effective batch size = 4*4 = 16
    max_grad_norm: 1.0

    # Performance optimization
    use_torch_compile: false  # Enable torch.compile() for performance (PyTorch 2.0+, not supported on Windows)
    torch_compile_mode: default  # default, reduce-overhead, max-autotune

    # Memory optimization
    use_fp16: false  # Mixed precision training (disable for max accuracy)
    dataloader_num_workers: 0  # Avoid multiprocessing overhead
    max_samples_per_task: null  # Limit samples per task (null = use all)
    use_streaming: true  # Streaming datasets (minimal RAM, slower but accurate)

    # Checkpoint management
    save_epoch_checkpoints: true  # Save checkpoint after each epoch
    auto_resume: true  # Automatically resume from latest checkpoint if available
    keep_all_epoch_checkpoints: false  # Keep all epoch checkpoints (true) or only latest (false)
